{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ptt-crawler v1.0.0",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPMf5U80jcgq79ZtAu4MO6q",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tingchun0113/ptt-crawler/blob/main/ptt_crawler_v1_0_0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zv0yseykGB5V"
      },
      "source": [
        "## Install and import dependencies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tsKXf8MCW0lq"
      },
      "source": [
        "!pip install requests_html\n",
        "import requests, urllib.parse, pandas as pd\n",
        "from requests_html import HTML\n",
        "from bs4 import BeautifulSoup\n",
        "from multiprocessing import Pool\n",
        "from google.colab import files"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bMzjfLoyFL7Q"
      },
      "source": [
        "## Variables"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ov9loSruXm17"
      },
      "source": [
        "board = 'Loan' #看板名稱\n",
        "domain = 'https://www.ptt.cc/'\n",
        "url = domain + 'bbs/' + board + '/index.html'\n",
        "\n",
        "num_pages = 5 #number of pages to be crawled\n",
        "title_keywords = ['桃園'] #e.g. title_keywords = ['房貸', '房屋'] (標題有 '房貸' 或 '房屋')"
      ],
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gCkdEIJMF4bF"
      },
      "source": [
        "## Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g-3TG7ZKX347"
      },
      "source": [
        "def fetch(url):\n",
        "  response = requests.get(url)\n",
        "  response = requests.get(url, cookies={'over18': '1'})\n",
        "  return response\n",
        "\n",
        "def parse_article_entries(doc, el): \n",
        "  html = HTML(html=doc)\n",
        "  post_entries = html.find(el)\n",
        "  return post_entries\n",
        "\n",
        "def parse_article_meta(entry):\n",
        "    meta = {\n",
        "      'title': entry.find('div.title', first=True).text,\n",
        "      'push': entry.find('div.nrec', first=True).text,\n",
        "      'date': entry.find('div.date', first=True).text\n",
        "    }\n",
        "    try:\n",
        "      meta['author'] = entry.find('div.author', first=True).text\n",
        "      meta['link'] = entry.find('div.title > a', first=True).attrs['href']\n",
        "    except AttributeError:\n",
        "      meta['author'] = '[Deleted]'\n",
        "      meta['link'] = '[Deleted]'\n",
        "    return meta\n",
        "\n",
        "def get_metadata_from(url):\n",
        "\n",
        "    def parse_next_link(doc):\n",
        "        html = HTML(html=doc)\n",
        "        controls = html.find('.action-bar a.btn.wide')\n",
        "        link = controls[1].attrs.get('href')\n",
        "        return urllib.parse.urljoin(domain, link)\n",
        "\n",
        "    res = fetch(url)\n",
        "    post_entries = parse_article_entries(res.text, 'div.r-ent')\n",
        "    next_link = parse_next_link(res.text)\n",
        "    \n",
        "    metadata = [parse_article_meta(entry) for entry in post_entries]\n",
        "    return metadata, next_link\n",
        "\n",
        "def get_paged_meta(url, num_pages):\n",
        "    collected_meta = []\n",
        "\n",
        "    for _ in range(num_pages):\n",
        "      posts, link = get_metadata_from(url)\n",
        "      collected_meta += posts\n",
        "      url = urllib.parse.urljoin(domain, link)\n",
        "\n",
        "    return collected_meta\n",
        "\n",
        "def create_fields(titles, dates, links, filtered_meta, meta):\n",
        "    titles.append(meta['title'])\n",
        "    dates.append(meta['date'])\n",
        "    links.append(urllib.parse.urljoin(domain, meta['link']))\n",
        "    filtered_meta.append(meta)\n",
        "\n",
        "    return titles, dates, links, filtered_meta\n",
        "\n",
        "def filter_metadata(url, num_pages):\n",
        "    titles = []\n",
        "    dates = []\n",
        "    links = []\n",
        "    filtered_meta = []\n",
        "\n",
        "    metadata = get_paged_meta(url, num_pages)\n",
        "    for meta in metadata:\n",
        "      if len(title_keywords) != 0:\n",
        "        for text in title_keywords: \n",
        "          if text in meta['title']:\n",
        "            create_fields(titles, dates, links, filtered_meta, meta)\n",
        "      else: \n",
        "        create_fields(titles, dates, links, filtered_meta, meta)\n",
        "\n",
        "    return titles, dates, links, filtered_meta\n",
        "\n",
        "def parse_content_data(main_container):\n",
        "    try:\n",
        "      pre_text = main_container.text.split('--')[0]\n",
        "      texts = pre_text.split('\\n')[2:]\n",
        "      data = '\\n'.join(texts)\n",
        "    except AttributeError:\n",
        "      data = '[Deleted]'\n",
        "    return data\n",
        "\n",
        "def parse_content_from(link):\n",
        "    res = fetch(link)\n",
        "    soup = BeautifulSoup(res.text, 'html.parser')\n",
        "    main_container = soup.find(id='main-container')\n",
        "    content_data = parse_content_data(main_container)\n",
        "    return content_data\n",
        "\n",
        "def get_contents(metadata):\n",
        "    contents = []\n",
        "    formatted_contents = []\n",
        "\n",
        "    post_links = [\n",
        "      urllib.parse.urljoin(domain, meta['link'])\n",
        "      for meta in metadata if 'link' in meta\n",
        "    ]\n",
        "\n",
        "    with Pool(processes=8) as pool:\n",
        "      contents += pool.map(parse_content_from, post_links)\n",
        "      return contents"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m7dHfXmbEGO4"
      },
      "source": [
        "## Preview crawled data\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H93IhsnSaJJH",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 142
        },
        "outputId": "832394f2-65e3-48da-9521-4c14e706a5c9"
      },
      "source": [
        "titles, dates, links, filtered_meta = filter_metadata(url, num_pages)\n",
        "contents = get_contents(filtered_meta)\n",
        "\n",
        "df = pd.DataFrame({'發文日期': dates, '標題': titles, '文章網址': links, '文章內容': contents})\n",
        "df"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>發文日期</th>\n",
              "      <th>標題</th>\n",
              "      <th>文章網址</th>\n",
              "      <th>文章內容</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>10/31</td>\n",
              "      <td>[問題] 新北跟桃園房貸</td>\n",
              "      <td>https://www.ptt.cc/bbs/Loan/M.1635680661.A.EE8...</td>\n",
              "      <td>[房屋資訊]\\n地點: 新北和桃園地區(機場捷運沿線)\\n類型： 電梯大樓\\n坪數: 未定，...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>11/01</td>\n",
              "      <td>[問題] 首購桃園房貸</td>\n",
              "      <td>https://www.ptt.cc/bbs/Loan/M.1635735875.A.C46...</td>\n",
              "      <td>\\n【房屋資訊】\\n地點：桃園平鎮區\\n類型：電梯大樓\\n坪數：權狀34.1坪\\n屋齡：24...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>10/30</td>\n",
              "      <td>[問題] 桃園中壢回復型房貸或理財型房貸</td>\n",
              "      <td>https://www.ptt.cc/bbs/Loan/M.1635582359.A.589...</td>\n",
              "      <td>[房屋資訊]\\n地點：桃園中壢中原大學附近\\n用途：投資\\n類型：電梯大樓/無車位/套房/2...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    發文日期  ...                                               文章內容\n",
              "0  10/31  ...  [房屋資訊]\\n地點: 新北和桃園地區(機場捷運沿線)\\n類型： 電梯大樓\\n坪數: 未定，...\n",
              "1  11/01  ...  \\n【房屋資訊】\\n地點：桃園平鎮區\\n類型：電梯大樓\\n坪數：權狀34.1坪\\n屋齡：24...\n",
              "2  10/30  ...  [房屋資訊]\\n地點：桃園中壢中原大學附近\\n用途：投資\\n類型：電梯大樓/無車位/套房/2...\n",
              "\n",
              "[3 rows x 4 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MkNyyITRei86"
      },
      "source": [
        "## Save to CSV"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "aT8_EqdOV-oj",
        "outputId": "037107e1-9e70-4fad-dfed-63a8ac5ad9c1"
      },
      "source": [
        "df.to_csv('ptt_data.csv')\n",
        "files.download('ptt_data.csv')"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_9f1eec48-f49d-4401-ae4d-aaf3e0a4c59f\", \"ptt_data.csv\", 115935)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ahRqG3Dhl4Td"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}